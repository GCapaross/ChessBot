{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1200a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, IterableDataset\n",
    "import chess\n",
    "from time import time\n",
    "import tqdm\n",
    "\n",
    "HEADERS = (\"bitmaps\", \"movePlayed\")\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e5bc8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class PositionEvaluatorNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(13, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(-1, 64 * 8 * 8)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return torch.tanh(self.fc2(x))  # Output between -1 and 1\n",
    "    \n",
    "\n",
    "class ChessBotNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 13 channels: 12 pieces + side to play (tensor[true's|false's])\n",
    "        self.conv1 = nn.Conv2d(13,64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(8 * 8 * 128, 256)\n",
    "        self.fc2 = nn.Linear(256, 64)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Initialize weights\n",
    "        nn.init.kaiming_uniform_(self.conv1.weight, nonlinearity='relu')\n",
    "        nn.init.kaiming_uniform_(self.conv2.weight, nonlinearity='relu')\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)  # Output raw logits\n",
    "        return x\n",
    "\n",
    "class CompleteChessBotNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 13 channels: 12 pieces + side to play (tensor[true's|false's])\n",
    "        self.conv1 = nn.Conv2d(13,64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(8 * 8 * 128, 256)\n",
    "        self.fc2 = nn.Linear(256, 64 * 63) # (Choose 2 squares from the board where the order matters) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        # Initialize weights\n",
    "        nn.init.kaiming_uniform_(self.conv1.weight, nonlinearity='relu')\n",
    "        nn.init.kaiming_uniform_(self.conv2.weight, nonlinearity='relu')\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sigmoid(self.conv1(x))\n",
    "        x = self.sigmoid(self.conv2(x))\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)  # Output raw logits\n",
    "        return x\n",
    "## Idea:\n",
    "##   One model that answers \"best piece to move in this position\"\n",
    "##   Then another model that answers \"best square to move piece X to\"\n",
    "\n",
    "# Add normalization after conv\n",
    "# Switch from relu to sigmoid or smth\n",
    "# Add more preprocessing by making the tensors there and avoid pre processing before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0338429a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "piece_to_idx = {\n",
    "    'P': 0, 'N': 1, 'B': 2, 'R': 3, 'Q': 4, 'K': 5,\n",
    "    'p': 6, 'n': 7, 'b': 8, 'r': 9, 'q': 10, 'k': 11\n",
    "}\n",
    "\n",
    "def board_to_tensor(board):\n",
    "    tensor = np.zeros((12, 8, 8), dtype=np.uint8)\n",
    "    for square in chess.SQUARES:\n",
    "        piece = board.piece_at(square)\n",
    "        if piece:\n",
    "            idx = piece_to_idx[piece.symbol()]\n",
    "            row = 7 - square // 8\n",
    "            col = square % 8\n",
    "            tensor[idx, row, col] = 1\n",
    "    return tensor\n",
    "\n",
    "def idxToUci(idx):\n",
    "    return chr(ord('a') + idx % 8) + str(idx // 8 + 1)\n",
    "\n",
    "def convert_to_array(row: str):\n",
    "    \"\"\"\"\n",
    "    Converts board into array\n",
    "\n",
    "    :param str row: Board with side to play;\n",
    "                        This must represent a np.ndarray[np.int64, shape=(13)] \n",
    "    :return: np.ndarray[shape(13, 8, 8), dtype=np.float32]]:\n",
    "    \"\"\"\n",
    "    boards = np.array(json.loads(row), dtype=np.uint64) # shape = (13,)\n",
    "    array = np.empty((13, 8, 8), dtype=np.uint8)\n",
    "\n",
    "    # Set pieces boards\n",
    "    board_int8_view = boards.view(dtype=np.uint8).reshape((13, 8)) # shape = (13, 8)\n",
    "    board_as_int = np.unpackbits(board_int8_view, axis=1).reshape((13, 8, 8))\n",
    "    array[:] = board_as_int\n",
    "\n",
    "    # Set side to play board\n",
    "    array[12] = np.ones(shape = (1, 8, 8)) if boards[12] == 1 else np.zeros(shape=(1,8,8))\n",
    "    return array\n",
    "\n",
    "\n",
    "letters = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\"]\n",
    "numbers = list(range(1, 10)) # [1..9]\n",
    "MOVE_DICTIONARY = {}\n",
    "cumulative = 0\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        for k in range(8):\n",
    "            for w in range (8):\n",
    "                if (i == k and j == w):\n",
    "                    cumulative += 1\n",
    "                    continue\n",
    "                from_square = f\"{letters[i]}{numbers[j]}\"\n",
    "                to_square = f\"{letters[k]}{numbers[w]}\"\n",
    "                MOVE_DICTIONARY[f\"{from_square}{to_square}\"] = (i * 8**3) + (j * 8**2) + (k * 8) + w - cumulative\n",
    "REVERSE_MOVE_DICTIONARY = {\n",
    "    value: key for key,value in MOVE_DICTIONARY.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e442ecda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "import polars as pl\n",
    "\n",
    "class ChessEvalDataset(Dataset):\n",
    "# class ChessEvalDataset(IterableDataset):\n",
    "    def __init__(self, file: str, model: Literal[\"pieces\", \"moves\"] = \"pieces\", load_batch_size = 6_400):\n",
    "        self.model = model\n",
    "        self.lazy_dataset = pl.scan_csv(file, has_header=False, new_columns=HEADERS)\n",
    "        self.batch_size = load_batch_size\n",
    "        self.feature_col = \"bitmaps\"\n",
    "        self.target_col = \"movePlayed\"\n",
    "        self.total_rows = self.lazy_dataset.select(pl.len()).collect().item()\n",
    "\n",
    "        self.cached_batches: dict[int, tuple] = {}\n",
    "        self.cached_batch_id: int | None = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total_rows\n",
    "    \n",
    "    def _get_batch(self, batch_id):\n",
    "        \"\"\"Load a specific batch of data, wrapped with lru_cache for memory management\"\"\"\n",
    "        # Calculate batch range\n",
    "        if batch_id == self.cached_batch_id:\n",
    "            return self.cached_batches[batch_id]\n",
    "        self.cached_batches.pop(self.cached_batch_id, None) # Delete old batch\n",
    "        \n",
    "        # Calculate batch range\n",
    "        start_idx = batch_id * self.batch_size\n",
    "        end_idx = min((batch_id + 1) * self.batch_size, self.total_rows)\n",
    "        \n",
    "        # Fetch only this batch of data using offset and limit\n",
    "        batch_dataset = (self.lazy_dataset\n",
    "                    .slice(start_idx, end_idx - start_idx)\n",
    "                    .collect())\n",
    "        \n",
    "        # Process features and target\n",
    "        features = batch_dataset.select(self.feature_col)\n",
    "        features = torch.tensor(np.array([convert_to_array(bitmaps) for bitmaps in features[\"bitmaps\"]]), dtype=torch.float32)\n",
    "        \n",
    "        # targets = batch_dataset.select(self.target_col).map_rows(lambda move_string: MOVE_DICTIONARY[move_string[0][:4]]).to_numpy()\n",
    "        targets = batch_dataset.select(self.target_col).to_numpy()\n",
    "        targets = torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "        self.cached_batch_id = batch_id\n",
    "        self.cached_batches[self.cached_batch_id] = (features, targets)\n",
    "        \n",
    "        return self.cached_batches[batch_id]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Calculate which batch this index belongs to\n",
    "        batch_id = idx // self.batch_size\n",
    "        # Get the batch\n",
    "        features, targets = self._get_batch(batch_id)\n",
    "        # Get the item from the batch\n",
    "        idx_in_batch = idx % self.batch_size\n",
    "        \n",
    "        return features[idx_in_batch], targets[idx_in_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7916a846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Using device:  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44798/44798 [02:34<00:00, 289.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 5.3297 - 4.2685 | Time: 154.72325229644775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 3580/44798 [00:20<02:11, 314.50it/s]"
     ]
    }
   ],
   "source": [
    "DATASET_PATH = '../dataset/processed/results.csv'\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "TRAINING_MODE = \"pieces\" # \"pieces\" or \"moves\"\n",
    "MODEL_WEIGHTS_OUTPUT_PATH = \"./models/CompleteModel_FINISHED.pth\"\n",
    "\n",
    "## !IMPORTANT: This dictates how much ram will be used, and how much data will be loaded\n",
    "# 1_280_000 loads around 5gb, dont push this too high as it will crash if ram deplects\n",
    "NUM_EXAMPLES_TO_LOAD_PER_FETCH = 1_280_000 \n",
    "\n",
    "test = ChessEvalDataset(file = DATASET_PATH, model=TRAINING_MODE, load_batch_size = NUM_EXAMPLES_TO_LOAD_PER_FETCH)\n",
    "loader = DataLoader(test, batch_size=64, shuffle=False, num_workers=0)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CompleteChessBotNetwork().to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "print(torch.cuda.is_available())\n",
    "print(\"Using device: \", device)\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    t0 = time()\n",
    "    running_loss = 0.0\n",
    "    for board_tensor_batch, target_eval_batch in tqdm.tqdm(loader):\n",
    "        board_tensor_batch_gpu, target_eval_batch_gpu = board_tensor_batch.to(device), target_eval_batch.to(device)  # Move data to GPU\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(board_tensor_batch_gpu)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(pred, target_eval_batch_gpu.squeeze(1))\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    tf = time()\n",
    "    print(f\"Epoch {epoch}, Loss: {loss.item():.4f} - {running_loss / len(loader):.4f} | Time: {tf-t0}\")\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        torch.save(model.state_dict(), f\"./models/CompleteModel_Epoch-{epoch+1}.pth\")       \n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), MODEL_WEIGHTS_OUTPUT_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

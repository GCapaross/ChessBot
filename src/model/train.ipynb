{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import chess\n",
    "import chess.pgn\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, IterableDataset\n",
    "\n",
    "from time import time\n",
    "import tqdm\n",
    "\n",
    "HEADERS = (\"bitmaps\", \"movePlayed\")\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8184abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_fens_from_pgn(pgn_file, label_from=\"result\"):\n",
    "    positions = []\n",
    "    with open(pgn_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        while True:\n",
    "            game = chess.pgn.read_game(f)\n",
    "            if game is None:\n",
    "                break\n",
    "\n",
    "            board = game.board()\n",
    "            result = game.headers.get(\"Result\")\n",
    "\n",
    "            # Label based on game outcome\n",
    "            if result == \"1-0\":\n",
    "                label = 1.0\n",
    "            elif result == \"0-1\":\n",
    "                label = -1.0\n",
    "            else:\n",
    "                label = 0.0\n",
    "\n",
    "            # Step through moves\n",
    "            for move in game.mainline_moves():\n",
    "                board.push(move)\n",
    "                fen = board.fen()\n",
    "                positions.append((fen, label))  # ← You can also call Stockfish here if you want\n",
    "\n",
    "    return positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e5bc8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class PositionEvaluatorNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(13, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = x.view(-1, 64 * 8 * 8)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return torch.tanh(self.fc2(x))  # Output between -1 and 1\n",
    "    \n",
    "\n",
    "class ChessBotNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 13 channels: 12 pieces + side to play (tensor[true's|false's])\n",
    "        self.conv1 = nn.Conv2d(13,64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(8 * 8 * 128, 256)\n",
    "        self.fc2 = nn.Linear(256, 64)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Initialize weights\n",
    "        nn.init.kaiming_uniform_(self.conv1.weight, nonlinearity='relu')\n",
    "        nn.init.kaiming_uniform_(self.conv2.weight, nonlinearity='relu')\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)  # Output raw logits\n",
    "        return x\n",
    "\n",
    "\n",
    "## OUTPUT COMPLETE MOVES\n",
    "class CompleteChessBotNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 13 channels: 12 pieces + side to play (tensor[true's|false's])\n",
    "        self.conv1 = nn.Conv2d(13,64, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(8 * 8 * 128, 256)\n",
    "        self.fc2 = nn.Linear(256, 64 * 63) # (Choose 2 squares from the board where the order matters) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "        # Initialize weights\n",
    "        nn.init.kaiming_uniform_(self.conv1.weight, nonlinearity='relu')\n",
    "        nn.init.kaiming_uniform_(self.conv2.weight, nonlinearity='relu')\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.sigmoid(self.conv1(x))\n",
    "        x = self.sigmoid(self.conv2(x))\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)  # Output raw logits\n",
    "        return x\n",
    "## Idea:\n",
    "##   One model that answers \"best piece to move in this position\"\n",
    "##   Then another model that answers \"best square to move piece X to\"\n",
    "\n",
    "# Add normalization after conv\n",
    "# Switch from relu to sigmoid or smth\n",
    "# Add more preprocessing by making the tensors there and avoid pre processing before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0338429a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "\n",
    "piece_to_idx = {\n",
    "    'P': 0, 'N': 1, 'B': 2, 'R': 3, 'Q': 4, 'K': 5,\n",
    "    'p': 6, 'n': 7, 'b': 8, 'r': 9, 'q': 10, 'k': 11\n",
    "}\n",
    "\n",
    "def board_to_tensor(board):\n",
    "    tensor = np.zeros((12, 8, 8), dtype=np.uint8)\n",
    "    for square in chess.SQUARES:\n",
    "        piece = board.piece_at(square)\n",
    "        if piece:\n",
    "            idx = piece_to_idx[piece.symbol()]\n",
    "            row = 7 - square // 8\n",
    "            col = square % 8\n",
    "            tensor[idx, row, col] = 1\n",
    "    return tensor\n",
    "\n",
    "def idxToUci(idx):\n",
    "    return chr(ord('a') + idx % 8) + str(idx // 8 + 1)\n",
    "\n",
    "\n",
    "letters = [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\", \"h\"]\n",
    "numbers = list(range(1, 10)) # [1..9]\n",
    "MOVE_DICTIONARY = {}\n",
    "cumulative = 0\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        for k in range(8):\n",
    "            for w in range (8):\n",
    "                if (i == k and j == w):\n",
    "                    cumulative += 1\n",
    "                    continue\n",
    "                from_square = f\"{letters[i]}{numbers[j]}\"\n",
    "                to_square = f\"{letters[k]}{numbers[w]}\"\n",
    "                MOVE_DICTIONARY[f\"{from_square}{to_square}\"] = (i * 8**3) + (j * 8**2) + (k * 8) + w - cumulative\n",
    "REVERSE_MOVE_DICTIONARY = {\n",
    "    value: key for key,value in MOVE_DICTIONARY.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e442ecda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "import polars as pl\n",
    "\n",
    "class ChessEvalDataset(Dataset):\n",
    "# class ChessEvalDataset(IterableDataset):\n",
    "    def __init__(self, file: str, model: Literal[\"pieces\", \"moves\"] = \"pieces\", load_batch_size = 6_400):\n",
    "        self.model = model\n",
    "        self.lazy_dataset = pl.scan_csv(file, has_header=False, new_columns=HEADERS)\n",
    "        self.batch_size = load_batch_size\n",
    "        self.feature_col = \"bitmaps\"\n",
    "        self.target_col = \"movePlayed\"\n",
    "        self.total_rows = self.lazy_dataset.select(pl.len()).collect().item()\n",
    "\n",
    "        self.cached_batches: dict[int, tuple] = {}\n",
    "        self.cached_batch_id: int | None = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total_rows\n",
    "    \n",
    "    def _get_batch(self, batch_id):\n",
    "        \"\"\"Load a specific batch of data, wrapped with lru_cache for memory management\"\"\"\n",
    "        # Calculate batch range\n",
    "        if batch_id == self.cached_batch_id:\n",
    "            return self.cached_batches[batch_id]\n",
    "        self.cached_batches.pop(self.cached_batch_id, None) # Delete old batch\n",
    "        \n",
    "        start_idx = batch_id * self.batch_size\n",
    "        end_idx = min((batch_id + 1) * self.batch_size, self.total_rows)\n",
    "        \n",
    "        # Fetch only this batch of data using offset and limit\n",
    "        batch_dataset = (self.lazy_dataset\n",
    "                    .slice(start_idx, end_idx - start_idx)\n",
    "                    .collect())\n",
    "        \n",
    "        # Process features and target\n",
    "        features = batch_dataset.select(self.feature_col).map_rows(\n",
    "            lambda row: (np.frombuffer(\n",
    "                row[0].encode(),\n",
    "                dtype=np.int64\n",
    "            ).reshape(13, 8, 8), )\n",
    "        ).to_numpy()\n",
    "\n",
    "        features = np.stack(features[:, 0])\n",
    "        features = torch.tensor(features, dtype=torch.float)\n",
    "\n",
    "        targets = batch_dataset.select(s                            sideToPlay,\n",
    "elf.target_col).to_numpy()\n",
    "        targets = torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "        self.cached_batch_id = batch_id\n",
    "        self.cached_batches[self.cached_batch_id] = (features, targets)\n",
    "        \n",
    "        return self.cached_batches[batch_id]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Calculate which batch this index belongs to\n",
    "        batch_id = idx // self.batch_size\n",
    "        # Get the batch\n",
    "        features, targets = self._get_batch(batch_id)\n",
    "        # Get the item from the batch\n",
    "        idx_in_batch = idx % self.batch_size\n",
    "        \n",
    "        return features[idx_in_batch], targets[idx_in_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7916a846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Using device:  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▎    | 23991/44798 [02:20<00:54, 380.32it/s] "
     ]
    }
   ],
   "source": [
    "DATASET_PATH = '../dataset/processed/npbytes_results_serialized.csv'\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "TRAINING_MODE = \"pieces\" # \"pieces\" or \"moves\"\n",
    "MODEL_WEIGHTS_OUTPUT_PATH = \"CompleteModel_SKIP_INITIAL.pth\"\n",
    "test = ChessEvalDataset(file = DATASET_PATH, model=TRAINING_MODE, load_batch_size = 192_000)\n",
    "loader = DataLoader(test, batch_size=64, shuffle=False, num_workers=0)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CompleteChessBotNetwork().to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "print(torch.cuda.is_available())\n",
    "print(\"Using device: \", device)\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    t0 = time()\n",
    "    running_loss = 0.0\n",
    "    for board_tensor_batch, target_eval_batch in tqdm.tqdm(loader):\n",
    "        board_tensor_batch_gpu, target_eval_batch_gpu = board_tensor_batch.to(device), target_eval_batch.to(device)  # Move data to GPU\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(board_tensor_batch_gpu)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(pred, target_eval_batch_gpu.squeeze(1))\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    tf = time()\n",
    "    print(f\"Epoch {epoch}, Loss: {loss.item():.4f} - {running_loss / len(loader):.4f} | Time: {tf-t0}\")\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        torch.save(model.state_dict(), f\"test_{epoch+1}.pth\")       \n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), MODEL_WEIGHTS_OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5334c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (2, 2)\n",
      "┌─────────────────────────────────┬──────────┐\n",
      "│ column_1                        ┆ column_2 │\n",
      "│ ---                             ┆ ---      │\n",
      "│ str                             ┆ i64      │\n",
      "╞═════════════════════════════════╪══════════╡\n",
      "│ \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000… ┆ 2303     │\n",
      "│ \u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000\u0000… ┆ 2682     │\n",
      "└─────────────────────────────────┴──────────┘\n",
      "[[[[0 0 0 ... 0 0 0]\n",
      "   [0 0 0 ... 0 0 0]\n",
      "   [0 0 0 ... 0 0 0]\n",
      "   ...\n",
      "   [0 0 0 ... 0 0 0]\n",
      "   [1 1 0 ... 1 1 1]\n",
      "   [0 0 0 ... 0 0 0]]\n",
      "\n",
      "  [[0 0 0 ... 0 0 0]\n",
      "   [0 0 0 ... 0 0 0]\n",
      "   [0 0 0 ... 0 0 0]\n",
      "   ...\n",
      "   [0 0 0 ... 1 0 0]\n",
      "   [0 0 0 ... 0 0 0]\n",
      "   [0 1 0 ... 0 0 0]]\n",
      "\n",
      "  [[0 0 0 ... 0 0 0]\n",
      "   [0 0 0 ... 0 0 0]\n",
      "   [0 0 0 ... 0 0 0]\n",
      "   ...\n",
      "   [0 0 0 ... 0 0 0]\n",
      "   [0 0 0 ... 0 0 0]\n",
      "   [0 0 1 ... 1 0 0]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0 0 0 ... 0 0 0]\n",
      "   [0 0 0 ... 0 0 0]\n",
      "   [0 0 0 ... 0 0 0]\n",
      "   ...\n",
      "   [0 0 0 ... 0 0 0]\n",
      "   [0 0 0 ... 0 0 0]\n",
      "   [0 0 0 ... 0 0 0]]\n",
      "\n",
      "  [[0 0 0 ... 0 0 0]\n",
      "   [0 0 0 ... 0 0 0]\n",
      "   [0 0 0 ... 0 0 0]\n",
      "   ...\n",
      "   [0 0 0 ... 0 0 0]\n",
      "   [0 0 0 ... 0 0 0]\n",
      "   [0 0 0 ... 0 0 0]]\n",
      "\n",
      "  [[1 1 1 ... 1 1 1]\n",
      "   [1 1 1 ... 1 1 1]\n",
      "   [1 1 1 ... 1 1 1]\n",
      "   ...\n",
      "   [1 1 1 ... 1 1 1]\n",
      "   [1 1 1 ... 1 1 1]\n",
      "   [1 1 1 ... 1 1 1]]]\n",
      "\n",
      "\n",
      " [[[0 0 0 ... 0 0 0]\n",
      "   [0 0 0 ... 0 0 0]\n",
      "   [0 0 0 ... 0 0 0]\n",
      "   ...\n",
      "   [0 0 0 ... 0 0 0]\n",
      "   [1 1 0 ... 1 1 1]\n",
      "   [0 0 0 ... 0 0 0]]\n",
      "\n",
      "  [[0 0 0 ... 0 0 0]\n",
      "   [0 0 0 ... 0 0 0]\n",
      "   [0 0 0 ... 0 0 0]\n",
      "   ...\n",
      "   [0 0 0 ... 0 0 0]\n",
      "   [0 0 0 ... 0 0 0]\n",
      "   [0 1 0 ... 0 0 0]]\n",
      "\n",
      "  [[0 0 0 ... 0 0 0]\n",
      "   [0 0 0 ... 0 0 0]\n",
      "   [0 0 0 ... 0 0 0]\n",
      "   ...\n",
      "   [0 0 0 ... 0 0 0]\n",
      "   [0 0 0 ... 0 0 0]\n",
      "   [0 0 1 ... 1 0 0]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0 0 0 ... 0 0 0]\n",
      "   [0 0 0 ... 0 0 0]\n",
      "   [0 0 0 ... 0 0 0]\n",
      "   ...\n",
      "   [0 0 0 ... 0 0 0]\n",
      "   [0 0 0 ... 0 0 0]\n",
      "   [0 0 0 ... 0 0 0]]\n",
      "\n",
      "  [[0 0 0 ... 0 0 0]\n",
      "   [0 0 0 ... 0 0 0]\n",
      "   [0 0 0 ... 0 0 0]\n",
      "   ...\n",
      "   [0 0 0 ... 0 0 0]\n",
      "   [0 0 0 ... 0 0 0]\n",
      "   [0 0 0 ... 0 0 0]]\n",
      "\n",
      "  [[0 0 0 ... 0 0 0]\n",
      "   [0 0 0 ... 0 0 0]\n",
      "   [0 0 0 ... 0 0 0]\n",
      "   ...\n",
      "   [0 0 0 ... 0 0 0]\n",
      "   [0 0 0 ... 0 0 0]\n",
      "   [0 0 0 ... 0 0 0]]]]\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "df = pl.read_csv('../dataset/processed/npbytes_results_serialized.csv', n_rows=2, has_header=False)\n",
    "print(df)\n",
    "features = df.select(\"column_1\").map_rows(\n",
    "    lambda row: (np.frombuffer(\n",
    "        row[0].encode(),  # DO NOT ASK!\n",
    "        dtype=np.int64\n",
    "    ).reshape(13, 8, 8), )\n",
    ").to_numpy()\n",
    "features = np.stack(features[:, 0])\n",
    "print(features)\n",
    "# print(np.shape(features.reshape((13,8,8))))\n",
    "# print(features.reshape((13,8,8)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

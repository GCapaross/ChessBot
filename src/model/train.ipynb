{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1200a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import tqdm\n",
    "import pickle\n",
    "\n",
    "# Load the move dictionary created during the data-processing\n",
    "MOVE_DICTIONARY = pickle.load(open(\"../dataset/processed/test_elite/test_move_dictionary.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66156df7",
   "metadata": {},
   "source": [
    "# Initialize Dataset and Dataload\n",
    "## Dataset\n",
    "Our dataset consists of around 10M moves, played by humans with Lichess ELO of at least 2100, to ensure quality moves, pre-processed in file `data_processing.ipynb`\n",
    "Our dataset is written in csv files, with just two columns `bitmaps` that represent the game state and `move played` that represents the game played by a human in that game state. \n",
    "\n",
    "As our dataset contains a very large number of examples we load it in batches of `NUM_EXAMPLES_TO_LOAD_PER_FETCH`, normally 640k examples at a time.\n",
    "This is imperative as fetching 1.2M examples will use around 5gb ram, and, since the training was made in a GPU with 6GB of VRAM, we couldn't load the entire dataset at once.\n",
    "\n",
    "This loaded batch is shuffled to prevent any inherent order or pattern in the data from affecting the training, especially sinced our data consists of moves of games that are read in order.\n",
    "\n",
    "## Dataloader\n",
    "Dataloader serves the purpose of fetching the data from our dataset and dividing the examples in batches of `TRAINING_BATCH_SIZE` elements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e442ecda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from ChessDataset import ChessEvalDataset\n",
    "\n",
    "DATASET_PATH = '../dataset/processed/test_elite/results_black.csv'\n",
    "## !IMPORTANT: This dictates how much ram will be used, and how much data will be loaded\n",
    "# 640_000 loads around 5gb, dont push this too high as it will crash if ram deplects\n",
    "# NUM_EXAMPLES_TO_LOAD_PER_FETCH = 1_280_000 \n",
    "NUM_EXAMPLES_TO_LOAD_PER_FETCH = 320_000\n",
    "TRAINING_BATCH_SIZE = 64\n",
    "\n",
    "HEADERS = (\"bitmaps\", \"movePlayed\")\n",
    "dataset = ChessEvalDataset(\n",
    "    file = DATASET_PATH, \n",
    "    validation_size = 25_000,\n",
    "    load_batch_size = NUM_EXAMPLES_TO_LOAD_PER_FETCH, \n",
    "    headers = HEADERS\n",
    ")\n",
    "dataloader = DataLoader(dataset, batch_size=TRAINING_BATCH_SIZE, shuffle=False) # Shuffle is made in the dataset manually"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b967d507",
   "metadata": {},
   "source": [
    "# Model\n",
    "We use a model, saved in the file `model.py`present in the folder `./model/models/architecture_2Conv/model.py` (this allowed us to have a versioning system of our models) \n",
    "\n",
    "The final model consists of two Convolutional layers and two fully connected layers, that receive a 8x8x12 tensor, which represent the game state (8x8 squares, 6 white pieces and 6 black pieces)\n",
    "\n",
    "And has 1800 outputs, each represent a played move (moves played in our dataset)\n",
    "We didn't used all possible moves (64x63 moves) because since we didn't have all the possible moves represented in our dataset our model wasn't converging to acceptable values (40% validation accuracy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acf001f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "import gc\n",
    "# from models.architecture_batchnorm_2Conv.model import CompleteChessBotNetwork\n",
    "from models.architecture_2Conv_classes.model import ChessModel as ChessModel\n",
    "model = ChessModel(len(MOVE_DICTIONARY)).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f021dc7",
   "metadata": {},
   "source": [
    "# Training Loop\n",
    "Our training loop, written in pseudocode:\n",
    "```\n",
    "For each epoch:\n",
    "    For batch in dataloader.get_next_batch():\n",
    "        bitmaps, expected_moves = batch\n",
    "        predictions = model.predict(bitmaps)\n",
    "\n",
    "        loss = CrossEntropyLoss(predictions, expected_moves)\n",
    "        loss.backpropagation()\n",
    "\n",
    "    validation_dataset = get_validation_dataset()\n",
    "    loss, accuracy = evaluate_accuracy(validation_dataset)\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        save_model(model)\n",
    "```\n",
    "We also save the weights of our model every 5 epochs.\n",
    "\n",
    "## Optimizer and Scheduler\n",
    "We use, as an optimizer, Adam (Adaptive Moment Estimation) optimizer, which adjust learning rates during training, as it works well with large datasets and complex models because it uses memory efficiently and adapts the learning rate for each parameter automatically.\n",
    "\n",
    "## Loss Function\n",
    "As a classification problem, we use Cross Entropy Loss to calculate the loss of each batch\n",
    "\n",
    "## Model Accuracy Evaluation\n",
    "To evaluate our model, we extract, in the beginning, 50k examples from the dataset that are never used in the training phase, which allows us to see how well our model generalizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7916a846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Using device:  cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Accuracy: 48.44%: 100%|██████████| 157405/157405 [14:10<00:00, 185.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - 1.8637 | Training Accuracy: 43.79%| Time: 850.9807343482971\n",
      "Validation set - accuracy: 40.09% | loss: 2.0427\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Accuracy: 35.94%: 100%|██████████| 157405/157405 [14:59<00:00, 174.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 - 1.8538 | Training Accuracy: 44.01%| Time: 900.6456513404846\n",
      "Validation set - accuracy: 39.97% | loss: 2.0430\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Accuracy: 45.31%: 100%|██████████| 157405/157405 [16:03<00:00, 163.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 - 1.8458 | Training Accuracy: 44.20%| Time: 964.7244350910187\n",
      "Validation set - accuracy: 40.08% | loss: 2.0432\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Accuracy: 43.75%: 100%|██████████| 157405/157405 [16:45<00:00, 156.62it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 - 1.8386 | Training Accuracy: 44.36%| Time: 1005.9593245983124\n",
      "Validation set - accuracy: 40.02% | loss: 2.0425\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Accuracy: 37.50%: 100%|██████████| 157405/157405 [16:08<00:00, 162.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 - 1.8325 | Training Accuracy: 44.52%| Time: 968.8990986347198\n",
      "Validation set - accuracy: 40.17% | loss: 2.0416\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Accuracy: 45.31%: 100%|██████████| 157405/157405 [16:02<00:00, 163.51it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 - 1.8268 | Training Accuracy: 44.65%| Time: 963.496907711029\n",
      "Validation set - accuracy: 40.20% | loss: 2.0452\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Accuracy: 48.44%: 100%|██████████| 157405/157405 [15:47<00:00, 166.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 - 1.8219 | Training Accuracy: 44.77%| Time: 948.1530303955078\n",
      "Validation set - accuracy: 39.96% | loss: 2.0444\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Accuracy: 48.44%: 100%|██████████| 157405/157405 [15:59<00:00, 164.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17 - 1.8174 | Training Accuracy: 44.87%| Time: 960.5497415065765\n",
      "Validation set - accuracy: 39.95% | loss: 2.0436\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Accuracy: 42.19%: 100%|██████████| 157405/157405 [16:13<00:00, 161.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 - 1.8135 | Training Accuracy: 44.99%| Time: 974.0424044132233\n",
      "Validation set - accuracy: 40.16% | loss: 2.0427\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Accuracy: 53.12%: 100%|██████████| 157405/157405 [15:01<00:00, 174.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19 - 1.8099 | Training Accuracy: 45.06%| Time: 901.8765630722046\n",
      "Validation set - accuracy: 40.08% | loss: 2.0463\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Accuracy: 40.62%: 100%|██████████| 157405/157405 [15:04<00:00, 173.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 - 1.8062 | Training Accuracy: 45.15%| Time: 905.5936632156372\n",
      "Validation set - accuracy: 40.04% | loss: 2.0491\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Accuracy: 51.56%: 100%|██████████| 157405/157405 [15:25<00:00, 170.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21 - 1.8032 | Training Accuracy: 45.23%| Time: 926.1206521987915\n",
      "Validation set - accuracy: 40.22% | loss: 2.0466\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Accuracy: 42.19%:  29%|██▊       | 44999/157405 [03:44<07:52, 238.11it/s] "
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 100\n",
    "OUTPUT_PATH = './models/architecture_2Conv_classes/blackOnly_batchnorm'\n",
    "\n",
    "# Continue with pretrained weights\n",
    "model.load_state_dict(torch.load(\"./models/architecture_2Conv_classes/blackOnly_batchnorm/epoch-10.pth\"))\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "print(\"Using device: \", device)\n",
    "for epoch in range(10, NUM_EPOCHS+1):\n",
    "    model.train()\n",
    "    t0 = time.time()\n",
    "    avg_loss = 0.0\n",
    "    correct = 0\n",
    "    for board_tensor, target_eval in (pbar := tqdm.tqdm(dataloader)):        \n",
    "        board_tensor, target_eval = board_tensor.to(device), target_eval.to(device)  # Move data to GPU\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(board_tensor)\n",
    "\n",
    "        # Compute loss with valid move vlaidaiton\n",
    "        loss = loss_fn(pred, target_eval.squeeze(1))\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        avg_loss += loss.item()\n",
    "\n",
    "        batch_correct = (pred.argmax(dim=1) == target_eval[:, 0]).sum().item()\n",
    "        correct += batch_correct\n",
    "        pbar.set_description(f\"Batch Accuracy: {batch_correct*100 / (TRAINING_BATCH_SIZE):.2f}%\")\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Validation set\n",
    "    model.eval()\n",
    "    validation_features, validation_targets = dataset.get_validation_set()\n",
    "    validation_features = validation_features.to(device)\n",
    "    validation_targets = validation_targets.to(device)\n",
    "    with torch.no_grad():\n",
    "        pred = model(validation_features)\n",
    "        validation_set_loss = loss_fn(pred, validation_targets.squeeze(1))\n",
    "        validation_set_correct = (pred.argmax(dim=1) == validation_targets[:, 0]).sum().item()\n",
    "        validation_set_accuracy = 100 * validation_set_correct / len(validation_targets)\n",
    "        pred = pred.cpu()\n",
    "        validation_features = validation_features.cpu()\n",
    "        validation_targets = validation_targets.cpu()\n",
    "\n",
    "    accuracy = 100 * correct / (len(dataloader) * TRAINING_BATCH_SIZE)\n",
    "    tf = time.time()\n",
    "    print(f\"Epoch {epoch} - {avg_loss / len(dataloader):.4f} | Training Accuracy: {accuracy:.2f}%| Time: {tf-t0}\")\n",
    "    print(f\"Validation set - accuracy: {validation_set_accuracy:.2f}% | loss: {validation_set_loss:.4f}\\n\")\n",
    "\n",
    "    # Free GPU memory\n",
    "    del validation_features, validation_targets, validation_set_loss, validation_set_accuracy\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    if epoch % 5 == 0:\n",
    "        torch.save(model.state_dict(), f\"{OUTPUT_PATH}/epoch-{epoch}.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
